{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f25af97-affb-45f9-aa6e-d64aa050c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: keras_applications in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (1.0.8)\n",
      "Requirement already satisfied: h5py in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from keras_applications) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from keras_applications) (1.20.3)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: tensorflow_datasets in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (4.5.2)\n",
      "Requirement already satisfied: dill in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (0.3.4)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (1.20.3)\n",
      "Requirement already satisfied: importlib-resources in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (5.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (2.26.0)\n",
      "Requirement already satisfied: six in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-metadata in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (1.7.0)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (4.62.3)\n",
      "Requirement already satisfied: promise in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: termcolor in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: absl-py in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow_datasets) (1.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from importlib-resources->tensorflow_datasets) (3.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in ./anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.56.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_applications\n",
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ebc8ce-f7e3-4a76-8e55-a18d41413939",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7594,
     "status": "ok",
     "timestamp": 1648739120012,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "57ebc8ce-f7e3-4a76-8e55-a18d41413939",
    "outputId": "c08b5517-74d4-4794-e5df-e60d48d5c58c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "#import tensorflow_addons as tfa\n",
    "from tensorflow_datasets import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "#from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "#from tensorflow.keras.applications.xception import Xception, preprocess_input \n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.experimental import CosineDecayRestarts\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from keras import backend as K\n",
    "\n",
    "# places \n",
    "#from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import layer_utils\n",
    "\n",
    "import time\n",
    "from matplotlib import pyplot as plt, rc\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "import shutil\n",
    "import re\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "\n",
    "from PIL import Image\n",
    "import csv\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler as MMS\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "#from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
    "\n",
    "# places365에 필요한 \n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import layer_utils\n",
    "import matplotlib.image as img\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69225264-0130-4b60-a1cb-550d56e66151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr  9 07:54:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   48C    P0    38W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   44C    P0    37W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   44C    P0    43W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   48C    P0    44W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ZbANvXBJPn2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93647,
     "status": "ok",
     "timestamp": 1648739224578,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "1ZbANvXBJPn2",
    "outputId": "84aa9d38-a779-460d-881f-1b5394754ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/image_used_crowdSourcing\n"
     ]
    }
   ],
   "source": [
    "cd '/home/ubuntu/image_used_crowdSourcing/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZOzkGIsEPjO2",
   "metadata": {
    "id": "ZOzkGIsEPjO2"
   },
   "source": [
    "<style>\n",
    "    span { color: #aba; }\n",
    "</style>\n",
    "## <span style=\"color:#cba\"> 이미지 가져오기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9Qs1X81FRq_g",
   "metadata": {
    "executionInfo": {
     "elapsed": 125596,
     "status": "ok",
     "timestamp": 1648739853759,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "9Qs1X81FRq_g"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/home/ubuntu/pair_df.csv')\n",
    "df.drop(columns='Unnamed: 0', inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "YUcByl0LVoGw",
   "metadata": {
    "id": "YUcByl0LVoGw"
   },
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(data = {'image':list(df['left_image'])+list(df['right_image'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "upa2CCV7Yfqv",
   "metadata": {
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1648739884313,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "upa2CCV7Yfqv"
   },
   "outputs": [],
   "source": [
    "image_size= (448, 448)\n",
    "image_size_channel = (224, 224, 3)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def prepro_image(filepath):\n",
    "    image = tf.io.read_file(filepath) # CPU나 GPU 사용할 때\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, image_size) \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "h7rWCBdPjjtd",
   "metadata": {
    "executionInfo": {
     "elapsed": 6154,
     "status": "ok",
     "timestamp": 1648740015507,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "h7rWCBdPjjtd"
   },
   "outputs": [],
   "source": [
    "left_path = tf.data.Dataset.from_tensor_slices(list(df['left_image']))\n",
    "left = left_path.map(prepro_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "right_path = tf.data.Dataset.from_tensor_slices(list(df['right_image']))\n",
    "right = right_path.map(prepro_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "binary_label = tf.data.Dataset.from_tensor_slices(list(df['label']))\n",
    "\n",
    "\n",
    "cat = []\n",
    "for i in list(df['label']):\n",
    "  if i==0:\n",
    "    cat.append(np.array([1,0]))\n",
    "  else:\n",
    "    cat.append(np.array([0,1]))\n",
    "\n",
    "cat_label=tf.data.Dataset.from_tensor_slices(np.array(cat))\n",
    "\n",
    "pair_images=tf.data.Dataset.zip((left, right))\n",
    "cat_binary_labels = tf.data.Dataset.zip((cat_label, cat_label, binary_label))\n",
    "dataset = tf.data.Dataset.zip((pair_images, cat_binary_labels))\n",
    "\n",
    "# validation\n",
    "ratio = 0.8\n",
    "train_val_dataset = dataset.take(round(len(df) * ratio))\n",
    "train_dataset = train_val_dataset.take(round(len(df) * ratio * ratio))\n",
    "val_dataset = train_val_dataset.skip(round(len(df) * ratio * ratio))\n",
    "test_images = dataset.skip(round(len(df) * ratio))\n",
    "test_dataset = dataset.skip(round(len(df) * ratio)) \n",
    "\n",
    "\n",
    "# shuffle\n",
    "batch_size = 32 \n",
    "drop_remainder = False\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=2000, reshuffle_each_iteration=True)\n",
    "val_dataset = val_dataset.shuffle(buffer_size=2000, reshuffle_each_iteration=True)\n",
    "\n",
    "train_dataset = train_dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "train_dataset = train_dataset.prefetch(batch_size) \n",
    "\n",
    "val_dataset = val_dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "val_dataset = val_dataset.prefetch(batch_size)\n",
    "\n",
    "test_images = test_images.batch(batch_size, drop_remainder=drop_remainder)\n",
    "test_images = test_images.prefetch(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_AIAXO9yUVZ7",
   "metadata": {
    "id": "_AIAXO9yUVZ7"
   },
   "source": [
    "<style>\n",
    "    span { color: #aba; }\n",
    "</style>\n",
    "## <span style=\"color:#cba\"> VGG_Places365\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "jBJoTos1UYyw",
   "metadata": {
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1648740136313,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "jBJoTos1UYyw"
   },
   "outputs": [],
   "source": [
    "def VGG16_Places365(include_top=True, weights='places',\n",
    "                    input_tensor=None, input_shape=None,\n",
    "                    pooling=None,\n",
    "                    classes=365):\n",
    "    \"\"\"Instantiates the VGG16-places365 architecture.\n",
    "    Optionally loads weights pre-trained\n",
    "    on Places. Note that when using TensorFlow,\n",
    "    for best performance you should set\n",
    "    `image_data_format=\"channels_last\"` in your Keras config\n",
    "    at ~/.keras/keras.json.\n",
    "    The model and the weights are compatible with both\n",
    "    TensorFlow and Theano. The data format\n",
    "    convention used by the model is the one\n",
    "    specified in your Keras config file.\n",
    "    # Arguments\n",
    "        include_top: whether to include the 3 fully-connected\n",
    "            layers at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "                 'places' (pre-training on Places),\n",
    "                 or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or `(3, 224, 244)` (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            \n",
    "            and width and height should be no smaller than 48.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`, or invalid input shape\n",
    "        \n",
    "        \"\"\"\n",
    "    WEIGHTS_PATH = 'https://github.com/GKalliatakis/Keras-VGG16-places365/releases/download/v1.0/vgg16-places365_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "    WEIGHTS_PATH_NO_TOP = 'https://github.com/GKalliatakis/Keras-VGG16-places365/releases/download/v1.0/vgg16-places365_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "    if not (weights in {'places', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `places` '\n",
    "                         '(pre-training on Places), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'places' and include_top and classes != 365:\n",
    "        raise ValueError('If using `weights` as places with `include_top`'\n",
    "                         ' as true, `classes` should be 365')\n",
    "\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=48,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten =include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    # Block 1\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block1_conv1')(img_input)\n",
    "\n",
    "    x = Conv2D(filters=64, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block1_conv2')(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"block1_pool\", padding='valid')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block2_conv1')(x)\n",
    "\n",
    "    x = Conv2D(filters=128, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block2_conv2')(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"block2_pool\", padding='valid')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block3_conv1')(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block3_conv2')(x)\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block3_conv3')(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"block3_pool\", padding='valid')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block4_conv1')(x)\n",
    "\n",
    "    x = Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block4_conv2')(x)\n",
    "\n",
    "    x = Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block4_conv3')(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"block4_pool\", padding='valid')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block5_conv1')(x)\n",
    "\n",
    "    x = Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block5_conv2')(x)\n",
    "\n",
    "    x = Conv2D(filters=512, kernel_size=3, strides=(1, 1), padding='same',\n",
    "               kernel_regularizer=l2(0.0002),\n",
    "               activation='relu', name='block5_conv3')(x)\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), name=\"block5_pool\", padding='valid')(x)\n",
    "\n",
    "    if include_top:\n",
    "        # Classification block\n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "        x = Dropout(0.5, name='drop_fc1')(x)\n",
    "\n",
    "        x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "        x = Dropout(0.5, name='drop_fc2')(x)\n",
    "        \n",
    "        x = Dense(365, activation='softmax', name=\"predictions\")(x)\n",
    "\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='vgg16-places365')\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'places':\n",
    "        if include_top:\n",
    "            weights_path = get_file('vgg16-places365_weights_tf_dim_ordering_tf_kernels.h5',\n",
    "                                    WEIGHTS_PATH,\n",
    "                                    cache_subdir='models')\n",
    "        else:\n",
    "            weights_path = get_file('vgg16-places365_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                                    WEIGHTS_PATH_NO_TOP,\n",
    "                                    cache_subdir='models')\n",
    "\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "        if K.backend() == 'theano':\n",
    "            layer_utils.convert_all_kernels_in_model(model)\n",
    "\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            if include_top:\n",
    "                maxpool = model.get_layer(name='block5_pool')\n",
    "                shape = maxpool.output_shape[1:]\n",
    "                dense = model.get_layer(name='fc1')\n",
    "                layer_utils.convert_dense_weights_data_format(dense, shape, 'channels_first')\n",
    "\n",
    "            if K.backend() == 'tensorflow':\n",
    "                warnings.warn('You are using the TensorFlow backend, yet you '\n",
    "                              'are using the Theano '\n",
    "                              'image data format convention '\n",
    "                              '(`image_data_format=\"channels_first\"`). '\n",
    "                              'For best performance, set '\n",
    "                              '`image_data_format=\"channels_last\"` in '\n",
    "                              'your Keras config '\n",
    "                              'at ~/.keras/keras.json.')\n",
    "\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679e382-7495-4974-a327-852dbe11ffc7",
   "metadata": {
    "id": "8679e382-7495-4974-a327-852dbe11ffc7"
   },
   "source": [
    "<style>\n",
    "    span { color: #aba; }\n",
    "</style>\n",
    "## <span style=\"color:#cba\"> RSS-CNN architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550c4c77-1dc6-4d70-9518-8fe9ef6955da",
   "metadata": {
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1648741271143,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "550c4c77-1dc6-4d70-9518-8fe9ef6955da"
   },
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.vgg16.preprocess_input\n",
    "\n",
    "def global_extraction(): # using ResNet50 network without FC layer\n",
    "\n",
    "    i = Input(shape=(224, 224, 3))\n",
    "    vgg_model = VGG16_Places365(weights = 'places',\n",
    "                                    include_top = False,\n",
    "                                    input_tensor=i)\n",
    "    \n",
    "    model = tf.keras.Sequential() \n",
    "\n",
    "    for layer in vgg_model.layers[:-1]:\n",
    "        model.add(layer)\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    for layer in model.layers[:-3]:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "  \n",
    "    return model\n",
    "\n",
    "def feature_extraction_network(): # using ResNet50 network without FC layer. 여기 고침\n",
    "\n",
    "    i = Input(shape=(224, 224, 3))\n",
    "    vgg_model = VGG16_Places365(weights = 'places',\n",
    "                                    include_top = False,\n",
    "                                    input_tensor=i)\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    for layer in vgg_model.layers[:-1]: \n",
    "        model.add(layer)\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    for layer in model.layers[:-3]:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = False\n",
    "  \n",
    "    return model\n",
    "\n",
    "def build_model(input_dim, feature_extraction_network, global_extraction):\n",
    "        \n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "      tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "      tf.keras.layers.experimental.preprocessing.RandomRotation(0.1)])\n",
    "    \n",
    "    input_left = Input(shape=input_dim, name='input_left')\n",
    "    input_right = Input(shape=input_dim, name='input_right')\n",
    "\n",
    "    # augmentation --> preprocess_input \n",
    "    aug_left = data_augmentation(input_left) # output shape (none, 448, 448, 3)\n",
    "    aug_right = data_augmentation(input_right)\n",
    "\n",
    "    # image resize - global에 대해서만\n",
    "    global_left = Lambda(lambda x: tf.image.resize(x, (224, 224)))(aug_left)\n",
    "    global_right = Lambda(lambda x: tf.image.resize(x, (224, 224)))(aug_right)   \n",
    "\n",
    "    # imge preprocess \n",
    "    \n",
    "    pp_glb_left = preprocess_input(global_left)\n",
    "    pp_glb_right = preprocess_input(global_right)\n",
    "\n",
    "    # patch\n",
    "    #feature_extraction_network = feature_extraction_network()\n",
    "\n",
    "    left_feature1 = feature_extraction_network(preprocess_input(aug_left[:, 0:224, 0:224, :]))\n",
    "    left_feature2 = feature_extraction_network(preprocess_input(aug_left[:, 0:224, 224:448, :]))\n",
    "    left_feature3 = feature_extraction_network(preprocess_input(aug_left[:, 224:448, 0:224, :]))\n",
    "    left_feature4 = feature_extraction_network(preprocess_input(aug_left[:, 224:448, 224:448, :]))\n",
    "\n",
    "    right_feature1 = feature_extraction_network(preprocess_input(aug_right[:, 0:224, 0:224, :]))\n",
    "    right_feature2 = feature_extraction_network(preprocess_input(aug_right[:, 0:224, 224:448, :]))\n",
    "    right_feature3 = feature_extraction_network(preprocess_input(aug_right[:, 224:448, 0:224, :]))\n",
    "    right_feature4 = feature_extraction_network(preprocess_input(aug_right[:, 224:448, 224:448, :]))\n",
    "\n",
    "    left_concat = Concatenate(name='left_concat')([left_feature1, left_feature2, left_feature3, left_feature4])\n",
    "    right_concat = Concatenate(name='right_concat')([right_feature1, right_feature2, right_feature3, right_feature4])\n",
    "\n",
    "    concat = Concatenate(name='feature_concat')([left_concat, right_concat])\n",
    "    x1 = Dropout(rate=0.5)(concat)\n",
    "    x1 = Dense(512, activation='relu')(x1)\n",
    "    x1 = Dropout(rate=0.5)(x1)\n",
    "    patches_loss = Dense(2, activation='softmax', name='patches')(x1)\n",
    "\n",
    "\n",
    "    # global\n",
    "    #global_extraction = feature_extraction_network()\n",
    "    left_global = global_extraction(pp_glb_left)\n",
    "    right_global = global_extraction(pp_glb_right)\n",
    "\n",
    "    global_concat = Concatenate()([left_global, right_global])\n",
    "    x2 = Dropout(rate=0.5)(global_concat)\n",
    "    x2 = Dense(256, activation='relu')(x2)\n",
    "    x2 = Dropout(rate=0.5)(x2)\n",
    "    glb_loss = Dense(2, activation='softmax', name='global')(x2)\n",
    "    \n",
    "    # score\n",
    "    left_total_concat = Concatenate()([left_concat, left_global])\n",
    "    right_total_concat = Concatenate()([right_concat, right_global])\n",
    "    \n",
    "    score_model = tf.keras.Sequential([Dropout(rate=0.5), Dense(256, activation='relu'), Dropout(rate=0.5), Dense(1)])\n",
    "    \n",
    "    left_score = score_model(left_total_concat)\n",
    "    right_score = score_model(right_total_concat)\n",
    "\n",
    "    diff = Subtract()([right_score, left_score])\n",
    "\n",
    "    # Pass difference through sigmoid function.\n",
    "    prob = Activation(\"sigmoid\", name='ranking')(diff)\n",
    "\n",
    "    # Build model.\n",
    "    model = Model(inputs = [input_left, input_right], outputs = [patches_loss, glb_loss, prob])\n",
    "    \n",
    "    return model, score_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12603563-bece-43cc-9f80-466f84eba570",
   "metadata": {
    "id": "12603563-bece-43cc-9f80-466f84eba570"
   },
   "source": [
    "<style>\n",
    "    span { color: #aba; }\n",
    "</style>\n",
    "## <span style=\"color:#cba\"> model training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "LblYa-PzaT1c",
   "metadata": {
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1648741206738,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "LblYa-PzaT1c"
   },
   "outputs": [],
   "source": [
    "# input_shape = (4, 28, 28, 3)\n",
    "input_dim = (448, 448, 3)\n",
    "#preprocess = preprocess()\n",
    "feature_extraction_network = feature_extraction_network()\n",
    "global_extraction = global_extraction()\n",
    "model, score_model = build_model(input_dim, feature_extraction_network, global_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b25f23de-a261-4448-8736-43b4eb194fea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_left (InputLayer)        [(None, 448, 448, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_right (InputLayer)       [(None, 448, 448, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " sequential_34 (Sequential)     (None, 448, 448, 3)  0           ['input_left[0][0]',             \n",
      "                                                                  'input_right[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_118 (  (None, 224, 224, 3)  0          ['sequential_34[0][0]']          \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_120 (  (None, 224, 224, 3)  0          ['sequential_34[0][0]']          \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_122 (  (None, 224, 224, 3)  0          ['sequential_34[0][0]']          \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_124 (  (None, 224, 224, 3)  0          ['sequential_34[0][0]']          \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_126 (  (None, 224, 224, 3)  0          ['sequential_34[1][0]']          \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_128 (  (None, 224, 224, 3)  0          ['sequential_34[1][0]']          \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_130 (  (None, 224, 224, 3)  0          ['sequential_34[1][0]']          \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_132 (  (None, 224, 224, 3)  0          ['sequential_34[1][0]']          \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_119 (  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_118[0]\n",
      " SlicingOpLambda)                                                [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_121 (  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_120[0]\n",
      " SlicingOpLambda)                                                [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_123 (  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_122[0]\n",
      " SlicingOpLambda)                                                [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_125 (  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_124[0]\n",
      " SlicingOpLambda)                                                [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_127 (  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_126[0]\n",
      " SlicingOpLambda)                                                [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_129 (  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_128[0]\n",
      " SlicingOpLambda)                                                [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_131 (  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_130[0]\n",
      " SlicingOpLambda)                                                [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_133 (  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_132[0]\n",
      " SlicingOpLambda)                                                [0]']                            \n",
      "                                                                                                  \n",
      " lambda_18 (Lambda)             (None, 224, 224, 3)  0           ['sequential_34[0][0]']          \n",
      "                                                                                                  \n",
      " lambda_19 (Lambda)             (None, 224, 224, 3)  0           ['sequential_34[1][0]']          \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_69 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_119[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_70 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_121[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_71 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_123[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_72 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_125[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_73 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_127[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_74 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_129[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_75 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_131[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_76 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_133[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_116 (  (None, 224, 224, 3)  0          ['lambda_18[0][0]']              \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_117 (  (None, 224, 224, 3)  0          ['lambda_19[0][0]']              \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " sequential_32 (Sequential)     (None, 512)          14714688    ['tf.nn.bias_add_69[0][0]',      \n",
      "                                                                  'tf.nn.bias_add_70[0][0]',      \n",
      "                                                                  'tf.nn.bias_add_71[0][0]',      \n",
      "                                                                  'tf.nn.bias_add_72[0][0]',      \n",
      "                                                                  'tf.nn.bias_add_73[0][0]',      \n",
      "                                                                  'tf.nn.bias_add_74[0][0]',      \n",
      "                                                                  'tf.nn.bias_add_75[0][0]',      \n",
      "                                                                  'tf.nn.bias_add_76[0][0]']      \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_67 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_116[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_68 (TFOpLambda)  (None, 224, 224, 3)  0          ['tf.__operators__.getitem_117[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " left_concat (Concatenate)      (None, 2048)         0           ['sequential_32[0][0]',          \n",
      "                                                                  'sequential_32[1][0]',          \n",
      "                                                                  'sequential_32[2][0]',          \n",
      "                                                                  'sequential_32[3][0]']          \n",
      "                                                                                                  \n",
      " right_concat (Concatenate)     (None, 2048)         0           ['sequential_32[4][0]',          \n",
      "                                                                  'sequential_32[5][0]',          \n",
      "                                                                  'sequential_32[6][0]',          \n",
      "                                                                  'sequential_32[7][0]']          \n",
      "                                                                                                  \n",
      " sequential_33 (Sequential)     (None, 512)          14714688    ['tf.nn.bias_add_67[0][0]',      \n",
      "                                                                  'tf.nn.bias_add_68[0][0]']      \n",
      "                                                                                                  \n",
      " feature_concat (Concatenate)   (None, 4096)         0           ['left_concat[0][0]',            \n",
      "                                                                  'right_concat[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate_30 (Concatenate)   (None, 1024)         0           ['sequential_33[0][0]',          \n",
      "                                                                  'sequential_33[1][0]']          \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 4096)         0           ['feature_concat[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 1024)         0           ['concatenate_30[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_32 (Concatenate)   (None, 2560)         0           ['right_concat[0][0]',           \n",
      "                                                                  'sequential_33[1][0]']          \n",
      "                                                                                                  \n",
      " concatenate_31 (Concatenate)   (None, 2560)         0           ['left_concat[0][0]',            \n",
      "                                                                  'sequential_33[0][0]']          \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 512)          2097664     ['dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 256)          262400      ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " sequential_35 (Sequential)     (None, 1)            655873      ['concatenate_31[0][0]',         \n",
      "                                                                  'concatenate_32[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 512)          0           ['dense_24[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_39 (Dropout)           (None, 256)          0           ['dense_25[0][0]']               \n",
      "                                                                                                  \n",
      " subtract_6 (Subtract)          (None, 1)            0           ['sequential_35[1][0]',          \n",
      "                                                                  'sequential_35[0][0]']          \n",
      "                                                                                                  \n",
      " patches (Dense)                (None, 2)            1026        ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      " global (Dense)                 (None, 2)            514         ['dropout_39[0][0]']             \n",
      "                                                                                                  \n",
      " ranking (Activation)           (None, 1)            0           ['subtract_6[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32,446,853\n",
      "Trainable params: 12,456,709\n",
      "Non-trainable params: 19,990,144\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6BbldY4JmFh",
   "metadata": {
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1648740697850,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "a6BbldY4JmFh"
   },
   "outputs": [],
   "source": [
    "make_folder = f'/home/ubuntu/train_0410_patch_aug_global_sep_again0409/'\n",
    "if not os.path.exists(make_folder):\n",
    "  os.makedirs(make_folder) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74a8fc6-ede6-4756-8c74-d0a4bdab4ba4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Downloading data from https://github.com/GKalliatakis/Keras-VGG16-places365/releases/download/v1.0/vgg16-places365_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58916864/58909656 [==============================] - 3s 0us/step\n",
      "58925056/58909656 [==============================] - 3s 0us/step\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/15\n",
      "INFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 2.5165 - patches_loss: 0.6828 - global_loss: 0.6982 - ranking_loss: 0.6657 - patches_accuracy: 0.6280 - global_accuracy: 0.6191 - ranking_accuracy: 0.6496\n",
      "Epoch 00001: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.01-2.140.hdf5\n",
      "2418/2418 [==============================] - 891s 275ms/step - loss: 2.5165 - patches_loss: 0.6828 - global_loss: 0.6982 - ranking_loss: 0.6657 - patches_accuracy: 0.6280 - global_accuracy: 0.6191 - ranking_accuracy: 0.6496 - val_loss: 2.1395 - val_patches_loss: 0.5572 - val_global_loss: 0.5652 - val_ranking_loss: 0.5480 - val_patches_accuracy: 0.7173 - val_global_accuracy: 0.7108 - val_ranking_accuracy: 0.7243\n",
      "Epoch 2/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 2.1791 - patches_loss: 0.5670 - global_loss: 0.5878 - ranking_loss: 0.5564 - patches_accuracy: 0.7102 - global_accuracy: 0.6971 - ranking_accuracy: 0.7211\n",
      "Epoch 00002: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.02-2.089.hdf5\n",
      "2418/2418 [==============================] - 651s 268ms/step - loss: 2.1791 - patches_loss: 0.5670 - global_loss: 0.5878 - ranking_loss: 0.5564 - patches_accuracy: 0.7102 - global_accuracy: 0.6971 - ranking_accuracy: 0.7211 - val_loss: 2.0887 - val_patches_loss: 0.5408 - val_global_loss: 0.5481 - val_ranking_loss: 0.5333 - val_patches_accuracy: 0.7340 - val_global_accuracy: 0.7254 - val_ranking_accuracy: 0.7364\n",
      "Epoch 3/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 2.1068 - patches_loss: 0.5431 - global_loss: 0.5651 - ranking_loss: 0.5345 - patches_accuracy: 0.7317 - global_accuracy: 0.7148 - ranking_accuracy: 0.7360\n",
      "Epoch 00003: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.03-2.062.hdf5\n",
      "2418/2418 [==============================] - 650s 268ms/step - loss: 2.1068 - patches_loss: 0.5431 - global_loss: 0.5651 - ranking_loss: 0.5345 - patches_accuracy: 0.7317 - global_accuracy: 0.7148 - ranking_accuracy: 0.7360 - val_loss: 2.0619 - val_patches_loss: 0.5328 - val_global_loss: 0.5398 - val_ranking_loss: 0.5278 - val_patches_accuracy: 0.7405 - val_global_accuracy: 0.7316 - val_ranking_accuracy: 0.7423\n",
      "Epoch 4/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 2.0618 - patches_loss: 0.5305 - global_loss: 0.5514 - ranking_loss: 0.5214 - patches_accuracy: 0.7396 - global_accuracy: 0.7253 - ranking_accuracy: 0.7447\n",
      "Epoch 00004: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.04-2.044.hdf5\n",
      "2418/2418 [==============================] - 651s 268ms/step - loss: 2.0618 - patches_loss: 0.5305 - global_loss: 0.5514 - ranking_loss: 0.5214 - patches_accuracy: 0.7396 - global_accuracy: 0.7253 - ranking_accuracy: 0.7447 - val_loss: 2.0443 - val_patches_loss: 0.5291 - val_global_loss: 0.5361 - val_ranking_loss: 0.5237 - val_patches_accuracy: 0.7409 - val_global_accuracy: 0.7368 - val_ranking_accuracy: 0.7469\n",
      "Epoch 5/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 2.0285 - patches_loss: 0.5216 - global_loss: 0.5396 - ranking_loss: 0.5152 - patches_accuracy: 0.7450 - global_accuracy: 0.7330 - ranking_accuracy: 0.7501\n",
      "Epoch 00005: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.05-2.035.hdf5\n",
      "2418/2418 [==============================] - 650s 268ms/step - loss: 2.0285 - patches_loss: 0.5216 - global_loss: 0.5396 - ranking_loss: 0.5152 - patches_accuracy: 0.7450 - global_accuracy: 0.7330 - ranking_accuracy: 0.7501 - val_loss: 2.0346 - val_patches_loss: 0.5275 - val_global_loss: 0.5360 - val_ranking_loss: 0.5218 - val_patches_accuracy: 0.7431 - val_global_accuracy: 0.7363 - val_ranking_accuracy: 0.7462\n",
      "Epoch 6/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 2.0009 - patches_loss: 0.5151 - global_loss: 0.5314 - ranking_loss: 0.5077 - patches_accuracy: 0.7493 - global_accuracy: 0.7388 - ranking_accuracy: 0.7545\n",
      "Epoch 00006: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.06-2.010.hdf5\n",
      "2418/2418 [==============================] - 650s 268ms/step - loss: 2.0009 - patches_loss: 0.5151 - global_loss: 0.5314 - ranking_loss: 0.5077 - patches_accuracy: 0.7493 - global_accuracy: 0.7388 - ranking_accuracy: 0.7545 - val_loss: 2.0103 - val_patches_loss: 0.5220 - val_global_loss: 0.5275 - val_ranking_loss: 0.5164 - val_patches_accuracy: 0.7451 - val_global_accuracy: 0.7404 - val_ranking_accuracy: 0.7495\n",
      "Epoch 7/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.9742 - patches_loss: 0.5075 - global_loss: 0.5242 - ranking_loss: 0.4998 - patches_accuracy: 0.7532 - global_accuracy: 0.7436 - ranking_accuracy: 0.7583\n",
      "Epoch 00007: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.07-2.001.hdf5\n",
      "2418/2418 [==============================] - 650s 268ms/step - loss: 1.9742 - patches_loss: 0.5075 - global_loss: 0.5242 - ranking_loss: 0.4998 - patches_accuracy: 0.7532 - global_accuracy: 0.7436 - ranking_accuracy: 0.7583 - val_loss: 2.0013 - val_patches_loss: 0.5199 - val_global_loss: 0.5260 - val_ranking_loss: 0.5146 - val_patches_accuracy: 0.7463 - val_global_accuracy: 0.7434 - val_ranking_accuracy: 0.7505\n",
      "Epoch 8/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.9581 - patches_loss: 0.5026 - global_loss: 0.5203 - ranking_loss: 0.4960 - patches_accuracy: 0.7561 - global_accuracy: 0.7465 - ranking_accuracy: 0.7629\n",
      "Epoch 00008: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.08-1.999.hdf5\n",
      "2418/2418 [==============================] - 652s 268ms/step - loss: 1.9581 - patches_loss: 0.5026 - global_loss: 0.5203 - ranking_loss: 0.4960 - patches_accuracy: 0.7561 - global_accuracy: 0.7465 - ranking_accuracy: 0.7629 - val_loss: 1.9988 - val_patches_loss: 0.5210 - val_global_loss: 0.5247 - val_ranking_loss: 0.5154 - val_patches_accuracy: 0.7467 - val_global_accuracy: 0.7433 - val_ranking_accuracy: 0.7509\n",
      "Epoch 9/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.9418 - patches_loss: 0.4997 - global_loss: 0.5151 - ranking_loss: 0.4907 - patches_accuracy: 0.7590 - global_accuracy: 0.7496 - ranking_accuracy: 0.7648\n",
      "Epoch 00009: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.09-1.987.hdf5\n",
      "2418/2418 [==============================] - 652s 269ms/step - loss: 1.9418 - patches_loss: 0.4997 - global_loss: 0.5151 - ranking_loss: 0.4907 - patches_accuracy: 0.7590 - global_accuracy: 0.7496 - ranking_accuracy: 0.7648 - val_loss: 1.9872 - val_patches_loss: 0.5184 - val_global_loss: 0.5218 - val_ranking_loss: 0.5119 - val_patches_accuracy: 0.7454 - val_global_accuracy: 0.7454 - val_ranking_accuracy: 0.7528\n",
      "Epoch 10/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.9265 - patches_loss: 0.4968 - global_loss: 0.5094 - ranking_loss: 0.4863 - patches_accuracy: 0.7613 - global_accuracy: 0.7518 - ranking_accuracy: 0.7674\n",
      "Epoch 00010: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.10-1.988.hdf5\n",
      "2418/2418 [==============================] - 651s 268ms/step - loss: 1.9265 - patches_loss: 0.4968 - global_loss: 0.5094 - ranking_loss: 0.4863 - patches_accuracy: 0.7613 - global_accuracy: 0.7518 - ranking_accuracy: 0.7674 - val_loss: 1.9875 - val_patches_loss: 0.5186 - val_global_loss: 0.5224 - val_ranking_loss: 0.5136 - val_patches_accuracy: 0.7475 - val_global_accuracy: 0.7461 - val_ranking_accuracy: 0.7533\n",
      "Epoch 11/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.9099 - patches_loss: 0.4925 - global_loss: 0.5037 - ranking_loss: 0.4817 - patches_accuracy: 0.7624 - global_accuracy: 0.7563 - ranking_accuracy: 0.7695\n",
      "Epoch 00011: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.11-1.982.hdf5\n",
      "2418/2418 [==============================] - 652s 269ms/step - loss: 1.9099 - patches_loss: 0.4925 - global_loss: 0.5037 - ranking_loss: 0.4817 - patches_accuracy: 0.7624 - global_accuracy: 0.7563 - ranking_accuracy: 0.7695 - val_loss: 1.9815 - val_patches_loss: 0.5170 - val_global_loss: 0.5216 - val_ranking_loss: 0.5118 - val_patches_accuracy: 0.7475 - val_global_accuracy: 0.7465 - val_ranking_accuracy: 0.7548\n",
      "Epoch 12/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.8982 - patches_loss: 0.4890 - global_loss: 0.4997 - ranking_loss: 0.4792 - patches_accuracy: 0.7657 - global_accuracy: 0.7581 - ranking_accuracy: 0.7713\n",
      "Epoch 00012: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.12-1.973.hdf5\n",
      "2418/2418 [==============================] - 650s 268ms/step - loss: 1.8982 - patches_loss: 0.4890 - global_loss: 0.4997 - ranking_loss: 0.4792 - patches_accuracy: 0.7657 - global_accuracy: 0.7581 - ranking_accuracy: 0.7713 - val_loss: 1.9730 - val_patches_loss: 0.5140 - val_global_loss: 0.5199 - val_ranking_loss: 0.5094 - val_patches_accuracy: 0.7479 - val_global_accuracy: 0.7463 - val_ranking_accuracy: 0.7540\n",
      "Epoch 13/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.8817 - patches_loss: 0.4857 - global_loss: 0.4936 - ranking_loss: 0.4734 - patches_accuracy: 0.7680 - global_accuracy: 0.7610 - ranking_accuracy: 0.7759\n",
      "Epoch 00013: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.13-1.974.hdf5\n",
      "2418/2418 [==============================] - 652s 269ms/step - loss: 1.8817 - patches_loss: 0.4857 - global_loss: 0.4936 - ranking_loss: 0.4734 - patches_accuracy: 0.7680 - global_accuracy: 0.7610 - ranking_accuracy: 0.7759 - val_loss: 1.9738 - val_patches_loss: 0.5150 - val_global_loss: 0.5192 - val_ranking_loss: 0.5114 - val_patches_accuracy: 0.7494 - val_global_accuracy: 0.7494 - val_ranking_accuracy: 0.7557\n",
      "Epoch 14/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.8732 - patches_loss: 0.4831 - global_loss: 0.4909 - ranking_loss: 0.4715 - patches_accuracy: 0.7693 - global_accuracy: 0.7630 - ranking_accuracy: 0.7762\n",
      "Epoch 00014: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.14-1.975.hdf5\n",
      "2418/2418 [==============================] - 650s 268ms/step - loss: 1.8732 - patches_loss: 0.4831 - global_loss: 0.4909 - ranking_loss: 0.4715 - patches_accuracy: 0.7693 - global_accuracy: 0.7630 - ranking_accuracy: 0.7762 - val_loss: 1.9750 - val_patches_loss: 0.5161 - val_global_loss: 0.5212 - val_ranking_loss: 0.5107 - val_patches_accuracy: 0.7512 - val_global_accuracy: 0.7473 - val_ranking_accuracy: 0.7562\n",
      "Epoch 15/15\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.8620 - patches_loss: 0.4800 - global_loss: 0.4876 - ranking_loss: 0.4679 - patches_accuracy: 0.7716 - global_accuracy: 0.7658 - ranking_accuracy: 0.7786\n",
      "Epoch 00015: saving model to /home/ubuntu/train_0410_patch_aug_global_sep/weights.15-1.968.hdf5\n",
      "2418/2418 [==============================] - 650s 268ms/step - loss: 1.8620 - patches_loss: 0.4800 - global_loss: 0.4876 - ranking_loss: 0.4679 - patches_accuracy: 0.7716 - global_accuracy: 0.7658 - ranking_accuracy: 0.7786 - val_loss: 1.9681 - val_patches_loss: 0.5138 - val_global_loss: 0.5194 - val_ranking_loss: 0.5089 - val_patches_accuracy: 0.7504 - val_global_accuracy: 0.7481 - val_ranking_accuracy: 0.7560\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    # with strategy.scope():\n",
    "    input_dim = (448, 448, 3)\n",
    "    feature_extraction_network = feature_extraction_network()\n",
    "    global_extraction = global_extraction()\n",
    "    model, score_model = build_model(input_dim, feature_extraction_network, global_extraction)\n",
    "\n",
    "    model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00001), \n",
    "    loss = [\"categorical_crossentropy\", \"categorical_crossentropy\", 'binary_crossentropy'],\n",
    "    metrics='accuracy'\n",
    "    )\n",
    "    mcp_cb = ModelCheckpoint(filepath=f'{make_folder}/'+'weights.{epoch:02d}-{val_loss:.3f}.hdf5', monitor='val_loss',\n",
    "                        save_best_only=False, save_weights_only=True, mode='min', period=1, verbose=1)\n",
    "    #rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=5, patience=2, mode='min', verbose=1)\n",
    "    # ely_cb = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
    "    #model.fit(x=[np.array([left]), np.array([right])], y=[np.array([np.array([0, 1])]), np.array([1])], epochs=1, validation_split=0.0, batch_size=None)\n",
    "    history = model.fit(train_dataset, \n",
    "          epochs=15, \n",
    "          validation_data=val_dataset,\n",
    "          callbacks=[mcp_cb])\n",
    "          # validation_split은 dataset은 사용하지 못한다\n",
    "\n",
    "    #model.fit(x=[np.array([left]), np.array([right])], y=[np.array([0, 1]), np.array([1])], epochs=10, validation_split=0.0, batch_size=None)\n",
    "    #model.fit(x=pair_batch, y=[cat_label, binary_label], epochs=10, validation_split=0.0, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "xwBCU2Ypaglq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "executionInfo": {
     "elapsed": 2072710,
     "status": "error",
     "timestamp": 1648743348506,
     "user": {
      "displayName": "jiyeon Kim",
      "userId": "02674522773313632904"
     },
     "user_tz": -540
    },
    "id": "xwBCU2Ypaglq",
    "outputId": "e81a7354-5581-4dd8-837b-b8bf75a0c772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/5\n",
      "INFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 20 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.9285 - patches_loss: 0.4987 - global_loss: 0.5095 - ranking_loss: 0.4868 - patches_accuracy: 0.7590 - global_accuracy: 0.7531 - ranking_accuracy: 0.7673\n",
      "Epoch 00001: saving model to /home/ubuntu/train_0410_patch_aug_global_sep_again0409/weights.01-1.984.hdf5\n",
      "2418/2418 [==============================] - 708s 277ms/step - loss: 1.9285 - patches_loss: 0.4987 - global_loss: 0.5095 - ranking_loss: 0.4868 - patches_accuracy: 0.7590 - global_accuracy: 0.7531 - ranking_accuracy: 0.7673 - val_loss: 1.9836 - val_patches_loss: 0.5172 - val_global_loss: 0.5224 - val_ranking_loss: 0.5115 - val_patches_accuracy: 0.7472 - val_global_accuracy: 0.7433 - val_ranking_accuracy: 0.7513\n",
      "Epoch 2/5\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.9166 - patches_loss: 0.4942 - global_loss: 0.5065 - ranking_loss: 0.4843 - patches_accuracy: 0.7628 - global_accuracy: 0.7545 - ranking_accuracy: 0.7689\n",
      "Epoch 00002: saving model to /home/ubuntu/train_0410_patch_aug_global_sep_again0409/weights.02-1.985.hdf5\n",
      "2418/2418 [==============================] - 657s 271ms/step - loss: 1.9166 - patches_loss: 0.4942 - global_loss: 0.5065 - ranking_loss: 0.4843 - patches_accuracy: 0.7628 - global_accuracy: 0.7545 - ranking_accuracy: 0.7689 - val_loss: 1.9850 - val_patches_loss: 0.5177 - val_global_loss: 0.5232 - val_ranking_loss: 0.5133 - val_patches_accuracy: 0.7450 - val_global_accuracy: 0.7438 - val_ranking_accuracy: 0.7506\n",
      "Epoch 3/5\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.8998 - patches_loss: 0.4898 - global_loss: 0.5003 - ranking_loss: 0.4797 - patches_accuracy: 0.7664 - global_accuracy: 0.7586 - ranking_accuracy: 0.7727\n",
      "Epoch 00003: saving model to /home/ubuntu/train_0410_patch_aug_global_sep_again0409/weights.03-1.982.hdf5\n",
      "2418/2418 [==============================] - 655s 270ms/step - loss: 1.8998 - patches_loss: 0.4898 - global_loss: 0.5003 - ranking_loss: 0.4797 - patches_accuracy: 0.7664 - global_accuracy: 0.7586 - ranking_accuracy: 0.7727 - val_loss: 1.9816 - val_patches_loss: 0.5169 - val_global_loss: 0.5229 - val_ranking_loss: 0.5126 - val_patches_accuracy: 0.7472 - val_global_accuracy: 0.7432 - val_ranking_accuracy: 0.7503\n",
      "Epoch 4/5\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.8874 - patches_loss: 0.4864 - global_loss: 0.4968 - ranking_loss: 0.4756 - patches_accuracy: 0.7669 - global_accuracy: 0.7593 - ranking_accuracy: 0.7731\n",
      "Epoch 00004: saving model to /home/ubuntu/train_0410_patch_aug_global_sep_again0409/weights.04-1.974.hdf5\n",
      "2418/2418 [==============================] - 657s 271ms/step - loss: 1.8874 - patches_loss: 0.4864 - global_loss: 0.4968 - ranking_loss: 0.4756 - patches_accuracy: 0.7669 - global_accuracy: 0.7593 - ranking_accuracy: 0.7731 - val_loss: 1.9743 - val_patches_loss: 0.5137 - val_global_loss: 0.5212 - val_ranking_loss: 0.5115 - val_patches_accuracy: 0.7496 - val_global_accuracy: 0.7449 - val_ranking_accuracy: 0.7539\n",
      "Epoch 5/5\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.8766 - patches_loss: 0.4842 - global_loss: 0.4931 - ranking_loss: 0.4719 - patches_accuracy: 0.7698 - global_accuracy: 0.7632 - ranking_accuracy: 0.7757\n",
      "Epoch 00005: saving model to /home/ubuntu/train_0410_patch_aug_global_sep_again0409/weights.05-1.970.hdf5\n",
      "2418/2418 [==============================] - 657s 271ms/step - loss: 1.8766 - patches_loss: 0.4842 - global_loss: 0.4931 - ranking_loss: 0.4719 - patches_accuracy: 0.7698 - global_accuracy: 0.7632 - ranking_accuracy: 0.7757 - val_loss: 1.9703 - val_patches_loss: 0.5149 - val_global_loss: 0.5181 - val_ranking_loss: 0.5104 - val_patches_accuracy: 0.7507 - val_global_accuracy: 0.7463 - val_ranking_accuracy: 0.7542\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "    # with strategy.scope():\n",
    "    input_dim = (448, 448, 3)\n",
    "    feature_extraction_network = feature_extraction_network()\n",
    "    global_extraction = global_extraction()\n",
    "    model, score_model = build_model(input_dim, feature_extraction_network, global_extraction)\n",
    "    \n",
    "    model.load_weights('/home/ubuntu/weights.09-1.990.hdf5') \n",
    "\n",
    "    model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00001), \n",
    "    loss = [\"categorical_crossentropy\", \"categorical_crossentropy\", 'binary_crossentropy'],\n",
    "    metrics='accuracy'\n",
    "    )\n",
    "    mcp_cb = ModelCheckpoint(filepath=f'{make_folder}/'+'weights.{epoch:02d}-{val_loss:.3f}.hdf5', monitor='val_loss',\n",
    "                        save_best_only=False, save_weights_only=True, mode='min', period=1, verbose=1)\n",
    "    #rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=5, patience=2, mode='min', verbose=1)\n",
    "    # ely_cb = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
    "    #model.fit(x=[np.array([left]), np.array([right])], y=[np.array([np.array([0, 1])]), np.array([1])], epochs=1, validation_split=0.0, batch_size=None)\n",
    "    history = model.fit(train_dataset, \n",
    "          epochs=5, \n",
    "          validation_data=val_dataset,\n",
    "          callbacks=[mcp_cb])\n",
    "          # validation_split은 dataset은 사용하지 못한다\n",
    "\n",
    "    #model.fit(x=[np.array([left]), np.array([right])], y=[np.array([0, 1]), np.array([1])], epochs=10, validation_split=0.0, batch_size=None)\n",
    "    #model.fit(x=pair_batch, y=[cat_label, binary_label], epochs=10, validation_split=0.0, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4919bc-655e-4cbf-b2df-bcaedb67325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/7\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 14 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2418/2418 [==============================] - ETA: 0s - loss: 1.3180 - feature_loss: 0.5493 - ranking_loss: 0.5556 - feature_accuracy: 0.7226 - ranking_accuracy: 0.7207"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "with mirrored_strategy.scope():\n",
    "# with strategy.scope():\n",
    "  input_dim = image_size_channel\n",
    "  feature_extraction_network = feature_extraction_network()\n",
    "  model, score_model = build_model(input_dim, feature_extraction_network)\n",
    "    \n",
    "  model.load_weights('/home/ubuntu/train_0401_KIN/weights.10-1.314.hdf5') \n",
    "\n",
    "  model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00005), \n",
    "    loss = [\"categorical_crossentropy\", 'binary_crossentropy'],\n",
    "    metrics='accuracy'\n",
    "    )\n",
    "  mcp_cb = ModelCheckpoint(filepath=f'{make_folder}/'+'weights.{epoch:02d}-{val_loss:.3f}.hdf5', monitor='val_loss',\n",
    "                        save_best_only=False, save_weights_only=True, mode='min', period=1, verbose=1)\n",
    "  #rlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=5, patience=2, mode='min', verbose=1)\n",
    "  # ely_cb = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
    "  #model.fit(x=[np.array([left]), np.array([right])], y=[np.array([np.array([0, 1])]), np.array([1])], epochs=1, validation_split=0.0, batch_size=None)\n",
    "  history = model.fit(\n",
    "          train_dataset, \n",
    "          epochs=7, \n",
    "          validation_data=val_dataset,\n",
    "          callbacks=[mcp_cb])\n",
    "          # validation_split은 dataset은 사용하지 못한다\n",
    "\n",
    "  #model.fit(x=[np.array([left]), np.array([right])], y=[np.array([0, 1]), np.array([1])], epochs=10, validation_split=0.0, batch_size=None)\n",
    "  #model.fit(x=pair_batch, y=[cat_label, binary_label], epochs=10, validation_split=0.0, batch_size=None)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [
    "_AIAXO9yUVZ7",
    "e-Atbpq7ajs7"
   ],
   "machine_shape": "hm",
   "name": "siamese.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
